{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joeydekruis/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data_original = pd.read_csv('../data/other/raw_signal_data.csv', index_col = 0)\n",
    "labels_data = pd.read_csv('../data/emotional_data/emotion_labels.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for combining the sentences and matching them with the right emotional value\n",
    "def stackRows():\n",
    "    # specify dictionary for results\n",
    "    dic = {\"averaged\": {}, \"arousal\": {}, \"valence\": {}, \"features\": {}}\n",
    "    # Specify the number of features we have\n",
    "    num_cols = len(data_original.loc[:, \"Fp1\":].columns)\n",
    "    \n",
    "    # Loop through respondents\n",
    "    for resp in np.unique(data_original[\"respondent\"]):\n",
    "        # Loop through sentences\n",
    "        for item in np.unique(data_original[data_original[\"respondent\"] == resp][\".id\"]):\n",
    "            # Create id that is represented by Id.[respondent].[sentence]. to locate the label\n",
    "            labelId = [i for i, el in enumerate(labels_data[\"ids\"]) if \"Id.{}.{}.\".format(resp, item) in str(el)]\n",
    "            # Check if the labelId is not empty\n",
    "            if len(labelId) != 0:\n",
    "                labelId = labelId[0]\n",
    "                # Specify the sentenceId so we can merge the data based on the sentence\n",
    "                sentenceId = \"Sen.{}.{}.\".format(resp, labels_data[\"item\"][labelId])\n",
    "                # Check if label already exists\n",
    "                if sentenceId not in dic[\"features\"]:\n",
    "                    # Add features to dic if it doesn't exist\n",
    "                    dic[\"features\"][sentenceId] = np.array(data_original[(data_original[\"respondent\"] == resp) & \n",
    "                                                (data_original[\".id\"] == item)].iloc[:,3:]).reshape(1, -1, num_cols)\n",
    "                    # Add valence and arousal\n",
    "                    dic[\"arousal\"][sentenceId] = labels_data[\"arousal\"][labelId]\n",
    "                    dic[\"valence\"][sentenceId] = labels_data[\"valence\"][labelId]\n",
    "                    dic[\"averaged\"][sentenceId] = 1\n",
    "                else:\n",
    "                    # Merge features with already existing features\n",
    "                    temp_data = np.array(data_original[(data_original[\"respondent\"] == resp) & \n",
    "                                        (data_original[\".id\"] == item)].iloc[:,3:]).reshape(1, -1, num_cols)\n",
    "                    dic[\"features\"][sentenceId] = np.hstack((dic[\"features\"][sentenceId], temp_data))\n",
    "                    # Calculate average\n",
    "                    arousal = [labels_data[\"arousal\"][labelId], dic[\"arousal\"][sentenceId]]\n",
    "                    dic[\"arousal\"][sentenceId] = np.sum(arousal)\n",
    "                    valence = [labels_data[\"valence\"][labelId], dic[\"valence\"][sentenceId]]\n",
    "                    dic[\"valence\"][sentenceId] = np.sum(valence)\n",
    "                    dic[\"averaged\"][sentenceId] += 1\n",
    "    delete = []\n",
    "    # Chech which sentences just have an adjective or just a noun and delete them\n",
    "    for i in dic[\"averaged\"]:\n",
    "        if dic[\"averaged\"][i] == 1:\n",
    "            delete += [i]\n",
    "        if dic[\"averaged\"][i] == 3:\n",
    "            delete += [i]\n",
    "    for i in delete:\n",
    "        dic[\"averaged\"].pop(i, None)\n",
    "        dic[\"arousal\"].pop(i, None)\n",
    "        dic[\"valence\"].pop(i, None)\n",
    "        dic[\"features\"].pop(i, None)\n",
    "                \n",
    "    return dic\n",
    "            \n",
    "data = stackRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get result in the right frame\n",
    "features = [*data[\"features\"].values()][0]\n",
    "for i in data[\"features\"].keys():\n",
    "    if i != list(data[\"features\"].keys())[0]:\n",
    "        features = np.vstack((features,data[\"features\"][i]))\n",
    "        \n",
    "labels = np.vstack((np.array([*data[\"arousal\"].values()]), np.array([*data[\"valence\"].values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/other/features_raw.npy\", features)\n",
    "np.save(\"../data/other/labels_summed.npy\", labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
